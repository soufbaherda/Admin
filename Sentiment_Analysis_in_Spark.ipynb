{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soufbaherda/Admin/blob/master/Sentiment_Analysis_in_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v98M7CNqr8ik"
      },
      "source": [
        "## Import modules and create spark session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk8_wpbKsLBD",
        "outputId": "bcb82087-ffa4-48f5-aff6-2dafacd7cdb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=fa3dae398206dca4a5ad147e1d044273dd8e25fa9203d498119817eed2451122\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWDPbMRqr8in"
      },
      "outputs": [],
      "source": [
        "#import modules\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n",
        "\n",
        "#create Spark session\n",
        "appName = \"Sentiment Analysis in Spark\"\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local\") \\\n",
        "    .appName(\"Word Count\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ltg-fZEr8ip"
      },
      "source": [
        "## Read data file into Spark dataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "A2V-3yAwr8ip",
        "outputId": "dbd8f1bc-beee-4053-c447-ec94da54b8ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------+-----+\n",
            "| SentimentText                                                                         |Label|\n",
            "+---------------------------------------------------------------------------------------+-----+\n",
            "|Wow... Loved this place.                                                               |1    |\n",
            "|Crust is not good.                                                                     |0    |\n",
            "|Not tasty and the texture was just nasty.                                              |0    |\n",
            "|Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.|1    |\n",
            "|The selection on the menu was great and so were the prices.                            |1    |\n",
            "|Now I am getting angry and I want my damn pho.                                         |0    |\n",
            "|Honeslty it didn't taste THAT fresh.)                                                  |0    |\n",
            "+---------------------------------------------------------------------------------------+-----+\n",
            "only showing top 7 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#read csv file into dataFrame with automatically inferred schema\n",
        "tweets_csv = spark.read.csv('/content/Restaurant_Reviews.csv', inferSchema=True, header=True)\n",
        "tweets_csv.show(truncate=False, n=7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig6lT43_r8iq"
      },
      "source": [
        "## Select the related data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dDZmTAwr8iq",
        "outputId": "c76359f4-7f20-40bb-bc1c-4758e081e0cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------+-----+\n",
            "|SentimentText                                                                          |label|\n",
            "+---------------------------------------------------------------------------------------+-----+\n",
            "|Wow... Loved this place.                                                               |1    |\n",
            "|Crust is not good.                                                                     |0    |\n",
            "|Not tasty and the texture was just nasty.                                              |0    |\n",
            "|Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.|1    |\n",
            "|The selection on the menu was great and so were the prices.                            |1    |\n",
            "+---------------------------------------------------------------------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#select only \"SentimentText\" and \"Sentiment\" column, \n",
        "#and cast \"Sentiment\" column data into integer\n",
        "data = tweets_csv.select(col(\" SentimentText\").alias(\"SentimentText\"), col(\"Label\").alias(\"label\").cast(\"Int\"))\n",
        "data.show(truncate = False,n=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "df= data.select(F.translate(F.col(\"SentimentText\"), \".!?\", \"\").alias(\"SentimentText\"),\"label\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeFWwvrV0YTO",
        "outputId": "d941545a-b426-43e9-db24-9bdfbd8afefe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|       SentimentText|label|\n",
            "+--------------------+-----+\n",
            "|Wow Loved this place|    1|\n",
            "|   Crust is not good|    0|\n",
            "|Not tasty and the...|    0|\n",
            "|Stopped by during...|    1|\n",
            "|The selection on ...|    1|\n",
            "|Now I am getting ...|    0|\n",
            "|Honeslty it didn'...|    0|\n",
            "|The potatoes were...|    0|\n",
            "|The fries were gr...|    1|\n",
            "|       A great touch|    1|\n",
            "|Service was very ...|    1|\n",
            "|   Would not go back|    0|\n",
            "|The cashier had n...|    0|\n",
            "|I tried the Cape ...|    1|\n",
            "|I was disgusted b...|    0|\n",
            "|I was shocked bec...|    0|\n",
            "|  Highly recommended|    1|\n",
            "|Waitress was a li...|    0|\n",
            "|This place is not...|    0|\n",
            "| did not like at all|    0|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQmr1_nBr8ir"
      },
      "source": [
        "## Divide data into training and testing \n",
        "\n",
        "---\n",
        "\n",
        "data\n",
        "\n",
        "1.   Élément de liste\n",
        "2.   Élément de liste\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kKI9Jw88hD85"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYL2bx3cr8ir",
        "outputId": "a5f00671-86f9-4f34-f81c-3f88c623078d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data rows: 730 ; Testing data rows: 269\n"
          ]
        }
      ],
      "source": [
        "#divide data, 75% for training, 25% for testing\n",
        "#dividedData = data.randomSplit([0.75, 0.25]) \n",
        "dividedData = df.randomSplit([0.75, 0.25]) \n",
        "trainingData = dividedData[0] #index 0 = data training\n",
        "testingData = dividedData[1] #index 1 = data testing\n",
        "train_rows = trainingData.count()\n",
        "test_rows = testingData.count()\n",
        "print (\"Training data rows:\", train_rows, \"; Testing data rows:\", test_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVVoIINQr8ir"
      },
      "source": [
        "## Prepare training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy0U4X7Mr8is"
      },
      "source": [
        "Separate \"SentimentText\" into individual words using tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "sZOJbqtDr8is",
        "outputId": "9dbc2d3b-fd1f-49f9-feb9-9fd24804facf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------+-----+------------------------------------------------------------------------------------------------------+\n",
            "|SentimentText                                                                       |label|SentimentWords                                                                                        |\n",
            "+------------------------------------------------------------------------------------+-----+------------------------------------------------------------------------------------------------------+\n",
            "|\"I don't know what the big deal is about this place, but I won't be back \"\"ya'all\"\"\"|0    |[\"i, don't, know, what, the, big, deal, is, about, this, place,, but, i, won't, be, back, \"\"ya'all\"\"\"]|\n",
            "|\"It was extremely \"\"crumby\"\" and pretty tasteless\"                                  |0    |[\"it, was, extremely, \"\"crumby\"\", and, pretty, tasteless\"]                                            |\n",
            "|\"Service is quick and even \"\"to go\"\" orders are just like we like it\"               |1    |[\"service, is, quick, and, even, \"\"to, go\"\", orders, are, just, like, we, like, it\"]                  |\n",
            "+------------------------------------------------------------------------------------+-----+------------------------------------------------------------------------------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer(inputCol=\"SentimentText\", outputCol=\"SentimentWords\")\n",
        "tokenizedTrain = tokenizer.transform(trainingData)\n",
        "tokenizedTrain.show(truncate=False, n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhB8K5SYr8is"
      },
      "source": [
        "##Removing stop words (unimportant words to be features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "WjsFWE1ir8it",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb3a76a-c3e4-464c-b683-c7af4b522111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------+-----+------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
            "|SentimentText                                                                       |label|SentimentWords                                                                                        |MeaningfulWords                                                          |\n",
            "+------------------------------------------------------------------------------------+-----+------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
            "|\"I don't know what the big deal is about this place, but I won't be back \"\"ya'all\"\"\"|0    |[\"i, don't, know, what, the, big, deal, is, about, this, place,, but, i, won't, be, back, \"\"ya'all\"\"\"]|[\"i, know, big, deal, place,, back, \"\"ya'all\"\"\"]                         |\n",
            "|\"It was extremely \"\"crumby\"\" and pretty tasteless\"                                  |0    |[\"it, was, extremely, \"\"crumby\"\", and, pretty, tasteless\"]                                            |[\"it, extremely, \"\"crumby\"\", pretty, tasteless\"]                         |\n",
            "|\"Service is quick and even \"\"to go\"\" orders are just like we like it\"               |1    |[\"service, is, quick, and, even, \"\"to, go\"\", orders, are, just, like, we, like, it\"]                  |[\"service, quick, even, \"\"to, go\"\", orders, like, like, it\"]             |\n",
            "|\"That just SCREAMS \"\"LEGIT\"\" in my booksomethat's also pretty rare here in Vegas\"   |1    |[\"that, just, screams, \"\"legit\"\", in, my, booksomethat's, also, pretty, rare, here, in, vegas\"]       |[\"that, screams, \"\"legit\"\", booksomethat's, also, pretty, rare, vegas\"]  |\n",
            "|\"The burger I got the \"\"Gold Standard\"\" a $17 burger and was kind of disappointed\"  |0    |[\"the, burger, i, got, the, \"\"gold, standard\"\", a, $17, burger, and, was, kind, of, disappointed\"]    |[\"the, burger, got, \"\"gold, standard\"\", $17, burger, kind, disappointed\"]|\n",
            "+------------------------------------------------------------------------------------+-----+------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
        "                       outputCol=\"MeaningfulWords\")\n",
        "SwRemovedTrain = swr.transform(tokenizedTrain).na.drop(how=\"any\")\n",
        "SwRemovedTrain.show(truncate=False, n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8psSKhRr8it"
      },
      "source": [
        "Converting words feature into numerical feature. In Spark 2.2.1,it is implemented in HashingTF funtion using Austin Appleby's MurmurHash 3 algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PmSA-pHr8it",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "787ff001-deb6-468d-ff00-b7ceff4a3880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|     MeaningfulWords|            features|\n",
            "+-----+--------------------+--------------------+\n",
            "|    0|[\"i, know, big, d...|(262144,[82453,13...|\n",
            "|    0|[\"it, extremely, ...|(262144,[23071,75...|\n",
            "|    1|[\"service, quick,...|(262144,[19030,10...|\n",
            "|    1|[\"that, screams, ...|(262144,[23071,29...|\n",
            "|    0|[\"the, burger, go...|(262144,[20298,79...|\n",
            "+-----+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
        "numericTrainData = hashTF.transform(SwRemovedTrain).select(\n",
        "    'label', 'MeaningfulWords', 'features')\n",
        "numericTrainData.show(truncate=True, n=5)\n",
        "\n",
        "# Création d'un objet IDF\n",
        "idf = IDF(inputCol=\"features\", outputCol=\"idf_features\")\n",
        "\n",
        "# Calcul de l'inverse des fréquences documentaires (IDF)\n",
        "idfModel = idf.fit(numericTrainData)\n",
        "tfidf = idfModel.transform(numericTrainData)\n",
        "tfidf =tfidf.na.drop(how=\"any\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqQkft4ar8iu"
      },
      "source": [
        "## Train our classifier model using training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBmCk4s-r8iu",
        "outputId": "b629b42e-215c-4712-fa71-0961d3c9437a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training is done!\n"
          ]
        }
      ],
      "source": [
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", \n",
        "                        maxIter=10, regParam=0.01)\n",
        "model = lr.fit(tfidf)\n",
        "print (\"Training is done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aC1Psm_r8iu"
      },
      "source": [
        "## Prepare testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "rmWqrI2wr8iu",
        "outputId": "f621b2ed-7aaf-4376-d907-4c62435d7a16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Label|MeaningfulWords                                                                  |features                                                                                                                            |\n",
            "+-----+---------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|0    |[\"the, servers, went, back, forth, several, times,, even, much, \"\"are, helped\"\"\"]|(262144,[76764,108160,129074,132270,139371,146139,156484,174639,174966,216238,258219],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
            "|0    |[\"the, food, tasty, all,, say, \"\"real, traditional, hunan, style\"\"\"]             |(262144,[45585,68505,94822,107574,121133,171222,216238,223782,248966],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                        |\n",
            "+-----+---------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizedTest = tokenizer.transform(testingData)\n",
        "SwRemovedTest = swr.transform(tokenizedTest)\n",
        "numericTest = hashTF.transform(SwRemovedTest).select(\n",
        "    'Label', 'MeaningfulWords', 'features').na.drop(how=\"any\")\n",
        "numericTest.show(truncate=False, n=2)\n",
        "# Création d'un objet IDF\n",
        "idf = IDF(inputCol=\"features\", outputCol=\"idf_features\")\n",
        "\n",
        "# Calcul de l'inverse des fréquences documentaires (IDF)\n",
        "idfModel = idf.fit(numericTest)\n",
        "tfidf = idfModel.transform(numericTest)\n",
        "tfidf =tfidf.na.drop(how=\"any\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQVixxxYr8iu"
      },
      "source": [
        "## Predict testing data and calculate the accuracy model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "mBumDiocr8iv",
        "outputId": "e73ed997-cb93-4d36-919c-5cb746a19c32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------+----------+-----+\n",
            "|MeaningfulWords                                                                  |prediction|Label|\n",
            "+---------------------------------------------------------------------------------+----------+-----+\n",
            "|[\"the, servers, went, back, forth, several, times,, even, much, \"\"are, helped\"\"\"]|0.0       |0    |\n",
            "|[\"the, food, tasty, all,, say, \"\"real, traditional, hunan, style\"\"\"]             |1.0       |0    |\n",
            "|[#name]                                                                          |0.0       |1    |\n",
            "|[#name]                                                                          |0.0       |1    |\n",
            "+---------------------------------------------------------------------------------+----------+-----+\n",
            "only showing top 4 rows\n",
            "\n",
            "correct prediction: 190 , total data: 269 , accuracy: 0.7063197026022305\n"
          ]
        }
      ],
      "source": [
        "prediction = model.transform(tfidf)\n",
        "predictionFinal = prediction.select(\n",
        "    \"MeaningfulWords\", \"prediction\", \"Label\")\n",
        "predictionFinal.show(n=4, truncate = False)\n",
        "correctPrediction = predictionFinal.filter(\n",
        "    predictionFinal['prediction'] == predictionFinal['Label']).count()\n",
        "totalData = predictionFinal.count()\n",
        "print(\"correct prediction:\", correctPrediction, \", total data:\", totalData, \n",
        "      \", accuracy:\", correctPrediction/totalData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h5jiTxer8iv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
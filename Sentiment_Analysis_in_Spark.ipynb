{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soufbaherda/Admin/blob/master/Sentiment_Analysis_in_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v98M7CNqr8ik"
      },
      "source": [
        "## Import modules and create spark session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk8_wpbKsLBD",
        "outputId": "44c2a57d-30f8-46b1-bfa9-09043fe3a937"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=e84f60824def36e6e0b411a1377937a083432a0ce25fb9ef8458ddfd1bd001fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OWDPbMRqr8in"
      },
      "outputs": [],
      "source": [
        "#import modules\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover,IDF\n",
        "\n",
        "#create Spark session\n",
        "appName = \"Sentiment Analysis in Spark\"\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local\") \\\n",
        "    .appName(\"Word Count\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ltg-fZEr8ip"
      },
      "source": [
        "## Read data file into Spark dataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "scrolled": true,
        "id": "A2V-3yAwr8ip",
        "outputId": "33334718-8b89-4a0a-c7e3-7c0096a856ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------+-----+\n",
            "| SentimentText                                                                         |Label|\n",
            "+---------------------------------------------------------------------------------------+-----+\n",
            "|Wow... Loved this place.                                                               |1    |\n",
            "|Crust is not good.                                                                     |0    |\n",
            "|Not tasty and the texture was just nasty.                                              |0    |\n",
            "|Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.|1    |\n",
            "|The selection on the menu was great and so were the prices.                            |1    |\n",
            "|Now I am getting angry and I want my damn pho.                                         |0    |\n",
            "|Honeslty it didn't taste THAT fresh.)                                                  |0    |\n",
            "+---------------------------------------------------------------------------------------+-----+\n",
            "only showing top 7 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#read csv file into dataFrame with automatically inferred schema\n",
        "tweets_csv = spark.read.csv('/content/Restaurant_Reviews.csv', inferSchema=True, header=True)\n",
        "tweets_csv.show(truncate=False, n=7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig6lT43_r8iq"
      },
      "source": [
        "## Select the related data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cette partie, nous avons selectionné juste les colonnes du text et label qu'on va prédire.\n",
        "Deplus nous avons renommé les colonnes afin d'utiliser facillement le code sur n'import quelle data, en unifiant le nom des colonnes. \n"
      ],
      "metadata": {
        "id": "Mr8AenXOxDbr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8dDZmTAwr8iq",
        "outputId": "b261e2a2-ca19-4263-b591-441e57fe64c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------+-----+\n",
            "|SentimentText                                                                          |label|\n",
            "+---------------------------------------------------------------------------------------+-----+\n",
            "|Wow... Loved this place.                                                               |1    |\n",
            "|Crust is not good.                                                                     |0    |\n",
            "|Not tasty and the texture was just nasty.                                              |0    |\n",
            "|Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.|1    |\n",
            "|The selection on the menu was great and so were the prices.                            |1    |\n",
            "+---------------------------------------------------------------------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#select only \"SentimentText\" and \"Sentiment\" column, \n",
        "#and cast \"Sentiment\" column data into integer\n",
        "data = tweets_csv.select(col(\" SentimentText\").alias(\"SentimentText\"), col(\"Label\").alias(\"label\").cast(\"Int\"))\n",
        "data.show(truncate = False,n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mini nettoyage des données"
      ],
      "metadata": {
        "id": "hl3jseSw04Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "df= data.select(F.translate(F.col(\"SentimentText\"), \".!?#NAME?\", \"\").alias(\"SentimentText\"),\"label\").na.drop(how=\"any\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeFWwvrV0YTO",
        "outputId": "3ce6eca7-9b42-4b8a-b662-40c3910a3548"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|       SentimentText|label|\n",
            "+--------------------+-----+\n",
            "|Wow Loved this place|    1|\n",
            "|   Crust is not good|    0|\n",
            "|ot tasty and the ...|    0|\n",
            "|Stopped by during...|    1|\n",
            "|The selection on ...|    1|\n",
            "|ow I am getting a...|    0|\n",
            "|Honeslty it didn'...|    0|\n",
            "|The potatoes were...|    0|\n",
            "|The fries were gr...|    1|\n",
            "|         great touch|    1|\n",
            "|Service was very ...|    1|\n",
            "|   Would not go back|    0|\n",
            "|The cashier had n...|    0|\n",
            "|I tried the Cape ...|    1|\n",
            "|I was disgusted b...|    0|\n",
            "|I was shocked bec...|    0|\n",
            "|  Highly recommended|    1|\n",
            "|Waitress was a li...|    0|\n",
            "|This place is not...|    0|\n",
            "| did not like at all|    0|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVVoIINQr8ir"
      },
      "source": [
        "## Prepare training data\n",
        "\n",
        "1.   Separate \"SentimentText\" into individual words using tokenizer\n",
        "2.   Removing stop words (unimportant words to be features)\n",
        "3.   Converting words feature into numerical feature. In Spark 2.2.1,it is implemented in HashingTF funtion using Austin Appleby's MurmurHash 3 algorithm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy0U4X7Mr8is"
      },
      "source": [
        "**Separate \"SentimentText\" into individual words using tokenizer **\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "scrolled": true,
        "id": "sZOJbqtDr8is",
        "outputId": "5efc8ca0-67d8-4376-cfaa-323158e1dd12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|SentimentText                                                                                                 |label|SentimentWords                                                                                                                       |\n",
            "+--------------------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Wow Loved this place                                                                                          |1    |[wow, loved, this, place]                                                                                                            |\n",
            "|Crust is not good                                                                                             |0    |[crust, is, not, good]                                                                                                               |\n",
            "|ot tasty and the texture was just nasty                                                                       |0    |[ot, tasty, and, the, texture, was, just, nasty]                                                                                     |\n",
            "|Stopped by during the late ay bank holiday off Rick Steve recommendation and loved it                         |1    |[stopped, by, during, the, late, ay, bank, holiday, off, rick, steve, recommendation, and, loved, it]                                |\n",
            "|The selection on the menu was great and so were the prices                                                    |1    |[the, selection, on, the, menu, was, great, and, so, were, the, prices]                                                              |\n",
            "|ow I am getting angry and I want my damn pho                                                                  |0    |[ow, i, am, getting, angry, and, i, want, my, damn, pho]                                                                             |\n",
            "|Honeslty it didn't taste THT fresh)                                                                           |0    |[honeslty, it, didn't, taste, tht, fresh)]                                                                                           |\n",
            "|The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer|0    |[the, potatoes, were, like, rubber, and, you, could, tell, they, had, been, made, up, ahead, of, time, being, kept, under, a, warmer]|\n",
            "|The fries were great too                                                                                      |1    |[the, fries, were, great, too]                                                                                                       |\n",
            "| great touch                                                                                                  |1    |[, great, touch]                                                                                                                     |\n",
            "+--------------------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer(inputCol=\"SentimentText\", outputCol=\"SentimentWords\")\n",
        "tokenizedTrain = tokenizer.transform(df).na.drop(how=\"any\")\n",
        "tokenizedTrain.show(truncate=False, n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhB8K5SYr8is"
      },
      "source": [
        "Removing stop words (unimportant words to be features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "scrolled": true,
        "id": "WjsFWE1ir8it",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a698d9e8-1f38-4796-dd9a-19bb8d00746f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------+\n",
            "|SentimentText                                                                                                 |label|SentimentWords                                                                                                                       |MeaningfulWords                                                       |\n",
            "+--------------------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------+\n",
            "|Wow Loved this place                                                                                          |1    |[wow, loved, this, place]                                                                                                            |[wow, loved, place]                                                   |\n",
            "|Crust is not good                                                                                             |0    |[crust, is, not, good]                                                                                                               |[crust, good]                                                         |\n",
            "|ot tasty and the texture was just nasty                                                                       |0    |[ot, tasty, and, the, texture, was, just, nasty]                                                                                     |[ot, tasty, texture, nasty]                                           |\n",
            "|Stopped by during the late ay bank holiday off Rick Steve recommendation and loved it                         |1    |[stopped, by, during, the, late, ay, bank, holiday, off, rick, steve, recommendation, and, loved, it]                                |[stopped, late, ay, bank, holiday, rick, steve, recommendation, loved]|\n",
            "|The selection on the menu was great and so were the prices                                                    |1    |[the, selection, on, the, menu, was, great, and, so, were, the, prices]                                                              |[selection, menu, great, prices]                                      |\n",
            "|ow I am getting angry and I want my damn pho                                                                  |0    |[ow, i, am, getting, angry, and, i, want, my, damn, pho]                                                                             |[ow, getting, angry, want, damn, pho]                                 |\n",
            "|Honeslty it didn't taste THT fresh)                                                                           |0    |[honeslty, it, didn't, taste, tht, fresh)]                                                                                           |[honeslty, taste, tht, fresh)]                                        |\n",
            "|The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer|0    |[the, potatoes, were, like, rubber, and, you, could, tell, they, had, been, made, up, ahead, of, time, being, kept, under, a, warmer]|[potatoes, like, rubber, tell, made, ahead, time, kept, warmer]       |\n",
            "|The fries were great too                                                                                      |1    |[the, fries, were, great, too]                                                                                                       |[fries, great]                                                        |\n",
            "| great touch                                                                                                  |1    |[, great, touch]                                                                                                                     |[, great, touch]                                                      |\n",
            "+--------------------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
        "                       outputCol=\"MeaningfulWords\")\n",
        "SwRemovedTrain = swr.transform(tokenizedTrain).na.drop(how=\"any\")\n",
        "SwRemovedTrain.show(truncate=False, n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8psSKhRr8it"
      },
      "source": [
        "Converting words feature into numerical feature. In Spark 2.2.1,it is implemented in HashingTF funtion using Austin Appleby's MurmurHash 3 algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "3PmSA-pHr8it",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0735c8-de74-4b67-ecae-3943facff3c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+--------------------+\n",
            "|label|     MeaningfulWords|            features|        idf_features|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "|    1| [wow, loved, place]|(262144,[4631,709...|(262144,[4631,709...|\n",
            "|    0|       [crust, good]|(262144,[113432,2...|(262144,[113432,2...|\n",
            "|    0|[ot, tasty, textu...|(262144,[21732,15...|(262144,[21732,15...|\n",
            "|    1|[stopped, late, a...|(262144,[53101,68...|(262144,[53101,68...|\n",
            "|    1|[selection, menu,...|(262144,[15370,12...|(262144,[15370,12...|\n",
            "|    0|[ow, getting, ang...|(262144,[12057,98...|(262144,[12057,98...|\n",
            "|    0|[honeslty, taste,...|(262144,[92393,18...|(262144,[92393,18...|\n",
            "|    0|[potatoes, like, ...|(262144,[14768,63...|(262144,[14768,63...|\n",
            "|    1|      [fries, great]|(262144,[171611,2...|(262144,[171611,2...|\n",
            "|    1|    [, great, touch]|(262144,[43333,24...|(262144,[43333,24...|\n",
            "|    1|   [service, prompt]|(262144,[43756,16...|(262144,[43756,16...|\n",
            "|    0|          [go, back]|(262144,[132270,1...|(262144,[132270,1...|\n",
            "|    0|[cashier, care, e...|(262144,[31536,10...|(262144,[31536,10...|\n",
            "|    1|[tried, cape, cod...|(262144,[18098,93...|(262144,[18098,93...|\n",
            "|    0|[disgusted, prett...|(262144,[23071,12...|(262144,[23071,12...|\n",
            "|    0|[shocked, signs, ...|(262144,[129941,1...|(262144,[129941,1...|\n",
            "|    1|[highly, recommen...|(262144,[13790,19...|(262144,[13790,19...|\n",
            "|    0|[waitress, little...|(262144,[27707,43...|(262144,[27707,43...|\n",
            "|    0|[place, worth, ti...|(262144,[27308,51...|(262144,[27308,51...|\n",
            "|    0|              [like]|(262144,[208258],...|(262144,[208258],...|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
        "numericTrainData = hashTF.transform(SwRemovedTrain).select(\n",
        "    'label', 'MeaningfulWords', 'features')\n",
        " \n",
        "# Création d'un objet IDF\n",
        "idf = IDF(inputCol=\"features\", outputCol=\"idf_features\")\n",
        "\n",
        "# Calcul de l'inverse des fréquences documentaires (IDF)\n",
        "idfModel = idf.fit(numericTrainData)\n",
        "tfidf = idfModel.transform(numericTrainData)\n",
        "tfidf =tfidf.na.drop(how=\"any\")\n",
        "tfidf.show(truncate=True,n=20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQmr1_nBr8ir"
      },
      "source": [
        "## Divide data into training and testing \n",
        "\n",
        "---\n",
        "\n",
        "data\n",
        "\n",
        "1.   Élément de liste\n",
        "2.   Élément de liste\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kKI9Jw88hD85"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "wYL2bx3cr8ir",
        "outputId": "e78c54fe-e31f-42a9-8b6b-99b3c8552368",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data rows: 724 ; Testing data rows: 273\n"
          ]
        }
      ],
      "source": [
        "#divide data, 75% for training, 25% for testing\n",
        "#dividedData = data.randomSplit([0.75, 0.25]) \n",
        "dividedData = tfidf.randomSplit([0.75, 0.25]) \n",
        "trainingData = dividedData[0] #index 0 = data training\n",
        "testingData = dividedData[1] #index 1 = data testing\n",
        "train_rows = trainingData.count()\n",
        "test_rows = testingData.count()\n",
        "print (\"Training data rows:\", train_rows, \"; Testing data rows:\", test_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqQkft4ar8iu"
      },
      "source": [
        "## Train our classifier model using training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "TBmCk4s-r8iu",
        "outputId": "57683b0e-f973-450d-bb29-a268250d48c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training is done!\n"
          ]
        }
      ],
      "source": [
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", \n",
        "                        maxIter=10, regParam=0.01)\n",
        "model = lr.fit(trainingData)\n",
        "print (\"Training is done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQVixxxYr8iu"
      },
      "source": [
        "## Predict testing data and calculate the accuracy model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "scrolled": true,
        "id": "mBumDiocr8iv",
        "outputId": "46864682-c4e8-41bc-b6e0-198be4336e11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct prediction: 201 , total data: 273 , accuracy: 0.7362637362637363\n"
          ]
        }
      ],
      "source": [
        "prediction = model.transform(testingData)\n",
        "predictionFinal = prediction.select(\n",
        "    \"MeaningfulWords\", \"prediction\", \"Label\")\n",
        "#predictionFinal.show(n=2, truncate = False)\n",
        "correctPrediction = predictionFinal.filter(\n",
        "    predictionFinal['prediction'] == predictionFinal['Label']).count()\n",
        "totalData = predictionFinal.count()\n",
        "print(\"correct prediction:\", correctPrediction, \", total data:\", totalData, \n",
        "      \", accuracy:\", correctPrediction/totalData)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}